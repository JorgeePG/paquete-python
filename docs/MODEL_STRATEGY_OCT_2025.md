# ğŸ¯ Estrategia de Modelos LLM - Octubre 2025

**OptimizaciÃ³n para Groq LPU Architecture**

---

## ğŸ“‹ **RESUMEN EJECUTIVO**

Sistema de selecciÃ³n dinÃ¡mica de modelos optimizado para la arquitectura LPU (Language Processing Units) de Groq, enfocado en modelos Meta que ofrecen el mejor rendimiento en esta plataforma.

### Resultados Clave:

- âœ… **17 llamadas** totales (16 fragmentos + 1 agregaciÃ³n)
- âœ… **~612 t/s** promedio ponderado
- âœ… **71% ahorro** en tokens de documentaciÃ³n API
- âœ… **Fallback automÃ¡tico** Tier 1 â†’ Tier 2 â†’ Tier 3

---

## ğŸ—ï¸ **ARQUITECTURA DE MODELOS**

### **Tier 1: UltrarrÃ¡pido (~800+ t/s)**

```
llama-3.1-8b-instant
â”œâ”€ Denso 8B parÃ¡metros
â”œâ”€ ~800+ tokens/segundo en Groq LPU
â”œâ”€ Ventana: 8,192 tokens
â””â”€ Uso: 37.5% de fragmentos (6/16)
    â”œâ”€ machines_summary
    â”œâ”€ operational_states
    â”œâ”€ system_properties (si <5000 chars)
    â”œâ”€ measurement_points (si <2000 chars)
    â”œâ”€ processing_modes (si <2000 chars)
    â””â”€ calculated_params (si <1000 chars)
```

### **Tier 2: Equilibrado MoE (~500 t/s)** â­ RECOMENDADO

```
meta-llama/llama-4-scout-17b-16e-instruct
â”œâ”€ Mixture of Experts (17B activos / 16 expertos)
â”œâ”€ ~500 tokens/segundo en Groq LPU
â”œâ”€ Ventana: 16,384 tokens â† Mayor contexto
â””â”€ Uso: 62.5% de fragmentos (10/16)
    â”œâ”€ system_properties (si â‰¥5000 chars)
    â”œâ”€ measurement_points (si â‰¥2000 chars)
    â”œâ”€ storage_strategies (medianos/grandes)
    â”œâ”€ processing_modes (â‰¥2000 chars, FFTs complejas)
    â”œâ”€ calculated_params (â‰¥1000 chars, alarmas)
    â””â”€ AgregaciÃ³n simple (â‰¤12 fragmentos)
```

### **Tier 3: Potente (~300 t/s)**

```
llama-3.3-70b-versatile
â”œâ”€ Denso 70B parÃ¡metros
â”œâ”€ ~300 tokens/segundo en Groq LPU
â”œâ”€ Ventana: 8,192 tokens
â””â”€ Uso: AgregaciÃ³n compleja
    â””â”€ SÃ­ntesis final (>12 fragmentos)
```

### **Fallbacks (Google)**

```
gemma-7b-it       (Tier 1, ~750 t/s, seguridad)
gemma2-9b-it      (Tier 2, ~450 t/s, alternativa)
```

---

## ğŸ”„ **ESTRATEGIA DE FALLBACK**

### Cadena AutomÃ¡tica de RecuperaciÃ³n:

```python
1. Modelo solicitado (ej: llama-3.1-8b-instant)
   â†“ [FALLA]
2. meta-llama/llama-4-scout-17b-16e-instruct (Tier 2)
   â†“ [FALLA]
3. llama-3.3-70b-versatile (Tier 3)
   â†“ [FALLA]
4. llama-3.1-8b-instant (Ãºltimo recurso)
   â†“ [FALLA]
5. RuntimeError con mensaje detallado
```

### CaracterÃ­sticas:

- âœ… **DeduplicaciÃ³n automÃ¡tica** (elimina duplicados en la cadena)
- âœ… **Preserva orden** (prioriza modelos mÃ¡s eficientes primero)
- âœ… **Solo modelos Groq LPU** (evita OpenAI GPT-OSS que no corren en LPU)
- âœ… **Mensajes de error informativos** (indica quÃ© fallÃ³ y dÃ³nde)

---

## ğŸ“Š **DISTRIBUCIÃ“N DE LLAMADAS**

Para un config.json con **16 fragmentos**:

| Tier  | Modelo               | Llamadas | %     | Velocidad | Total t/s |
| ----- | -------------------- | -------- | ----- | --------- | --------- |
| **1** | llama-3.1-8b-instant | 6        | 37.5% | ~800 t/s  | 300       |
| **2** | Scout MoE            | 10       | 62.5% | ~500 t/s  | 312.5     |
| **3** | llama-3.3-70b        | 1        | 5.9%  | ~300 t/s  | N/A\*     |

_AgregaciÃ³n final no se cuenta en promedio de fragmentos_

**Velocidad Promedio Ponderada:** ~612 tokens/segundo

---

## ğŸ¯ **SELECCIÃ“N DINÃMICA POR CHUNK TYPE**

### LÃ³gica de SelecciÃ³n (`ModelSelector.select_for_chunk_analysis`):

```python
SIMPLE (Tier 1 - 8B instant):
â”œâ”€ machines_summary
â”œâ”€ operational_states
â””â”€ Fragmentos <5000 chars

MEDIUM (Tier 2 - Scout):
â”œâ”€ measurement_points (â‰¥2000 chars)
â”œâ”€ storage_strategies (grandes)
â”œâ”€ processing_modes (FFTs complejas)
â””â”€ calculated_params (con alarmas)

COMPLEX (Tier 2 - Scout):
â””â”€ Fragmentos >4000 chars
    â””â”€ Scout maneja hasta 16k tokens
```

### AgregaciÃ³n (`ModelSelector.select_for_aggregation`):

```python
if num_fragments <= 12:
    return Scout (MoE, rÃ¡pido y suficiente)
else:
    return llama-3.3-70b (mÃ¡xima coherencia)
```

---

## ğŸ“„ **FRAGMENTACIÃ“N DE DOCUMENTACIÃ“N API**

### ApiDocFragmenter - 71% Ahorro de Tokens

**Antes:** ~1,250 tokens de DocComprimida.md por fragmento  
**DespuÃ©s:** ~360 tokens contextuales por fragmento

### Contextos Especializados:

| Chunk Type           | Contexto API                    | Tokens |
| -------------------- | ------------------------------- | ------ |
| `machines_summary`   | MÃ¡quinas, tags, alarmas         | 364    |
| `measurement_points` | Points, sensors, unidades       | 744    |
| `processing_modes`   | Proc modes, FFTs, parÃ¡metros    | 1,242  |
| `calculated_params`  | Params, alarmas, almacenamiento | 1,454  |
| `operational_states` | Estados operativos              | 450    |
| `storage_strategies` | Estrategias almacenamiento      | 520    |
| `system_properties`  | Propiedades globales            | 380    |

**Beneficio:** Cada fragmento recibe **solo la documentaciÃ³n relevante** para su tipo.

---

## ğŸš€ **IMPLEMENTACIÃ“N**

### Archivos Modificados:

1. **`src/llm_client/model_selector.py`**

   - âœ… Actualizado nombre Scout: `meta-llama/llama-4-scout-17b-16e-instruct`
   - âœ… LÃ³gica de selecciÃ³n por chunk type y tamaÃ±o
   - âœ… Estrategia de agregaciÃ³n (â‰¤12 â†’ Scout, >12 â†’ 70B)
   - âœ… MÃ©todo `get_model_stats()` actualizado

2. **`src/llm_client/groq_client.py`**

   - âœ… `_generate_completion()` con fallback automÃ¡tico
   - âœ… `get_available_models()` con modelos Groq LPU
   - âœ… EliminaciÃ³n de duplicados en cadena de fallback

3. **`src/llm_client/api_doc_fragmenter.py`**

   - âœ… 7 contextos especializados por chunk type
   - âœ… FragmentaciÃ³n inteligente de DocComprimida.md

4. **`src/llm_client/chunked_analyzer.py`**

   - âœ… IntegraciÃ³n ModelSelector para selecciÃ³n dinÃ¡mica
   - âœ… IntegraciÃ³n ApiDocFragmenter para contextos
   - âœ… CachÃ© con 24hr expiraciÃ³n

5. **`src/t8_client/cli.py`**
   - âœ… Comando `model-info` actualizado
   - âœ… Muestra arquitectura, rendimiento y estrategia

---

## ğŸ“ˆ **MÃ‰TRICAS DE EFICIENCIA**

### ComparaciÃ³n: Antes vs DespuÃ©s

| MÃ©trica                  | Antes    | DespuÃ©s  | Mejora  |
| ------------------------ | -------- | -------- | ------- |
| **Llamadas API**         | 39       | 17       | âœ… -56% |
| **Fragmentos**           | 38       | 16       | âœ… -58% |
| **Tokens API/fragmento** | ~1,250   | ~360     | âœ… -71% |
| **Velocidad promedio**   | ~400 t/s | ~612 t/s | âœ… +53% |
| **Modelos activos**      | 7        | 5        | âœ… -29% |

### Costo-Beneficio Tier 2 (Scout):

- **62.5% de llamadas** usan Scout (Tier 2)
- **16k tokens** de contexto (2x vs 8B)
- **~500 t/s** (mejor que 70B, suficiente calidad)
- **MoE 17B/16E:** activaciÃ³n eficiente de expertos

---

## ğŸ”§ **USO**

### Ver InformaciÃ³n de Modelos:

```bash
uv run t8-cli model-info
```

### AnÃ¡lisis con Modelo EspecÃ­fico:

```bash
# Usa estrategia automÃ¡tica (recomendado)
uv run t8-cli chat-config --verbose -i

# Limpia cachÃ© antes de analizar
uv run t8-cli chat-config --verbose -i --clear-cache
```

### ProgramÃ¡ticamente:

```python
from llm_client.model_selector import ModelSelector
from llm_client.chunking import ConfigChunk

# Seleccionar modelo para un fragmento
chunk = ConfigChunk(
    chunk_id="config_xyz_processing_modes_MAD31CY005_0",
    chunk_type="processing_modes",
    content_size=3500,
    data={"proc_modes": [...]}
)

model_config = ModelSelector.select_for_chunk_analysis(
    chunk_type=chunk.chunk_type,
    content_size=chunk.content_size
)

print(f"Modelo: {model_config.name}")
# Output: meta-llama/llama-4-scout-17b-16e-instruct

# Seleccionar para agregaciÃ³n
agg_model = ModelSelector.select_for_aggregation(
    num_fragments=16,
    total_size=50000
)
print(f"AgregaciÃ³n: {agg_model.name}")
# Output: llama-3.3-70b-versatile (porque 16 > 12)
```

---

## âš ï¸ **ADVERTENCIAS IMPORTANTES**

### âœ… **Nombres Exactos de Modelos**

Los nombres **DEBEN coincidir** con la API de Groq:

```python
# âœ… CORRECTO
"meta-llama/llama-4-scout-17b-16e-instruct"
"llama-3.1-8b-instant"
"llama-3.3-70b-versatile"

# âŒ INCORRECTO
"llama-4-scout-17b-16e-instruct"  # falta prefijo meta-llama/
"llama3-8b-instant"               # falta guiÃ³n y versiÃ³n
"llama-3.3-70b"                   # falta sufijo -versatile
```

### ğŸ”„ **ActualizaciÃ³n de Modelos**

Si Groq actualiza nombres o depreca modelos:

1. Verificar en [Groq Console](https://console.groq.com/playground)
2. Actualizar `ModelSelector.MODELS` en `model_selector.py`
3. Ejecutar `uv run t8-cli model-info` para validar

### ğŸš« **NO Usar Modelos OpenAI en Groq**

Evitar `openai/gpt-oss-*` porque:

- âŒ No corren en Groq LPU (infraestructura diferente)
- âŒ Rendimiento impredecible
- âŒ No optimizados para esta plataforma

---

## ğŸ“š **REFERENCIAS**

- [Groq LPU Architecture](https://groq.com/lpu/)
- [Meta Llama Models](https://llama.meta.com/)
- DocumentaciÃ³n interna: `llm/DocComprimida.md`
- CÃ³digo fuente: `src/llm_client/`

---

**Ãšltima actualizaciÃ³n:** Octubre 28, 2025  
**Optimizado para:** Groq LPU Architecture  
**Modelos principales:** Meta Llama 3.1, 3.3, 4 Scout
