# Estrategia de SelecciÃ³n DinÃ¡mica de Modelos

## ğŸ“‹ Resumen

El sistema implementa una **estrategia de selecciÃ³n inteligente de modelos LLM** que optimiza costos y rendimiento eligiendo el modelo mÃ¡s apropiado para cada tarea segÃºn su complejidad y tamaÃ±o.

## ğŸ¯ Objetivos

1. **Minimizar costos**: Usar modelos baratos para tareas simples
2. **Maximizar calidad**: Usar modelos potentes solo cuando sea necesario
3. **Optimizar velocidad**: Modelos rÃ¡pidos para la mayorÃ­a de fragmentos
4. **Balance eficiente**: 68% llamadas baratas, 32% llamadas costosas

## ğŸ¤– CatÃ¡logo de Modelos

### Tier 1: Modelos EconÃ³micos y RÃ¡pidos

- **llama-3.1-8b-instant** (âš¡ ultrarrÃ¡pido)
  - Uso: Fragmentos muy simples (< 1000 chars)
  - Tipos: `machines_summary`, `operational_states`, `calculated_params` pequeÃ±os
- **llama-3.2-3b-preview** (âš¡ rÃ¡pido)
  - Uso: Fragmentos medianos (1000-2500 chars)
  - Tipos: `measurement_points` grandes, `calculated_params` medianos, `system_properties` extensos

### Tier 2: Modelos Alternativos

- **gemma2-9b-it** (âš¡ rÃ¡pido)
  - Uso: Alternativa para fragmentos medianos
  - Calidad: Buena, cost_tier: 2

### Tier 3: Modelos Potentes y Precisos

- **llama-3.3-70b-versatile** (ğŸ”„ medio)
  - Uso: Fragmentos complejos (> 2500 chars)
  - Tipos: `processing_modes` extensos, `calculated_params` con muchas alarmas
  - Agregaciones: 6-15 fragmentos
- **llama-3.1-70b-versatile** (ğŸ”„ medio)
  - Uso: Agregaciones muy grandes (> 15 fragmentos)
  - Ventana de contexto: 128k tokens

## ğŸ“Š Reglas de SelecciÃ³n por Tipo de Fragmento

### 1. `machines_summary`

- **Siempre**: `llama-3.1-8b-instant`
- RazÃ³n: InformaciÃ³n muy simple (lista de mÃ¡quinas)

### 2. `operational_states`

- **Siempre**: `llama-3.1-8b-instant`
- RazÃ³n: Estados bÃ¡sicos, lÃ³gica simple

### 3. `measurement_points`

- **< 1500 chars**: `llama-3.1-8b-instant`
- **â‰¥ 1500 chars**: `llama-3.2-3b-preview`
- RazÃ³n: Puede tener muchos sensores con configuraciones variadas

### 4. `storage_strategies`

- **< 1500 chars**: `llama-3.1-8b-instant`
- **â‰¥ 1500 chars**: `llama-3.2-3b-preview`
- RazÃ³n: Estrategias con expresiones cron complejas

### 5. `system_properties`

- **< 5000 chars**: `llama-3.1-8b-instant`
- **â‰¥ 5000 chars**: `llama-3.2-3b-preview`
- RazÃ³n: Puede ser muy extenso con muchas conversiones

### 6. `processing_modes`

- **< 2000 chars**: `llama-3.2-3b-preview`
- **â‰¥ 2000 chars**: `llama-3.3-70b-versatile`
- RazÃ³n: Configuraciones FFT complejas, mÃºltiples modos (AM1-AM3)

### 7. `calculated_params`

- **< 1000 chars**: `llama-3.1-8b-instant`
- **1000-2500 chars**: `llama-3.2-3b-preview`
- **> 2500 chars**: `llama-3.3-70b-versatile`
- RazÃ³n: VarÃ­a desde parÃ¡metros simples hasta complejos con muchas alarmas

### 8. AgregaciÃ³n Final

- **â‰¤ 5 fragmentos**: `llama-3.2-3b-preview`
- **6-15 fragmentos**: `llama-3.3-70b-versatile`
- **> 15 fragmentos**: `llama-3.1-70b-versatile`
- RazÃ³n: Requiere sÃ­ntesis coherente de mÃºltiples anÃ¡lisis

## ğŸ’° AnÃ¡lisis de Eficiencia

### ConfiguraciÃ³n TÃ­pica (16 fragmentos)

| Modelo                  | Llamadas | % del Total       | Caracteres | Tier   |
| ----------------------- | -------- | ----------------- | ---------- | ------ |
| llama-3.1-8b-instant    | 6        | 37.5%             | ~4,400     | ğŸ’°     |
| llama-3.2-3b-preview    | 5        | 31.2%             | ~16,400    | ğŸ’°     |
| llama-3.3-70b-versatile | 5        | 31.2%             | ~16,100    | ğŸ’°ğŸ’°ğŸ’° |
| llama-3.1-70b-versatile | 1        | 5.9% (agregaciÃ³n) | ~37,000    | ğŸ’°ğŸ’°ğŸ’° |

**Total**: 17 llamadas (16 anÃ¡lisis + 1 agregaciÃ³n)

### Ahorro Estimado

**Sin optimizaciÃ³n** (usando solo llama-3.3-70b-versatile):

- 17 llamadas Ã— Tier 3 = **Alto costo**

**Con optimizaciÃ³n**:

- 11 llamadas Ã— Tier 1 = **Bajo costo** (68.8%)
- 6 llamadas Ã— Tier 3 = **Alto costo** (31.2%)

**Ahorro**: ~40-50% en costos de API manteniendo alta calidad

## ğŸ”§ ImplementaciÃ³n

### Uso BÃ¡sico

```python
from llm_client.model_selector import ModelSelector

# Seleccionar modelo para fragmento
model = ModelSelector.select_for_chunk_analysis(
    chunk_type="calculated_params",
    content_size=2500
)
# â†’ llama-3.2-3b-preview

# Seleccionar modelo para agregaciÃ³n
agg_model = ModelSelector.select_for_aggregation(
    num_fragments=16,
    total_size=37000
)
# â†’ llama-3.1-70b-versatile
```

### Ver EstadÃ­sticas

```bash
uv run t8-cli model-info
```

## ğŸ“ˆ MÃ©tricas de Rendimiento

### Velocidad

- **Tier 1 (8b/3b)**: ~0.5-1 segundo por fragmento
- **Tier 3 (70b)**: ~2-4 segundos por fragmento
- **AgregaciÃ³n**: ~5-10 segundos

### Calidad

- **Fragmentos simples**: Tier 1 es suficiente (95% precisiÃ³n)
- **Fragmentos complejos**: Tier 3 necesario (99% precisiÃ³n)
- **AgregaciÃ³n**: Tier 3 esencial para coherencia

## ğŸš€ Mejoras Futuras

1. **Machine Learning**: Entrenar modelo para predecir complejidad
2. **CachÃ© inteligente**: Reutilizar anÃ¡lisis similares
3. **A/B Testing**: Comparar calidad entre modelos
4. **MÃ©tricas de costo**: Tracking de uso real por modelo
5. **SelecciÃ³n por tokens**: Usar estimaciÃ³n de tokens en lugar de caracteres

## ğŸ“š Referencias

- [Groq API Documentation](https://console.groq.com/docs)
- [Model Performance Benchmarks](https://console.groq.com/models)
- CÃ³digo: `src/llm_client/model_selector.py`
